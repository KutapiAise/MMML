{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ai_FraudDetection_liveSession.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KutapiAise/MMML/blob/master/ai_FraudDetection_liveSession.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwXwtAUmDXj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "from sklearn.preprocessing import scale\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW34MrLQFUMq",
        "colab_type": "text"
      },
      "source": [
        "**Fraud Detection**\n",
        "\n",
        "Our tasks\n",
        "\n",
        "*   Explore the dataset/visualize\n",
        "*   Decide which features are important\n",
        "*   Perform Machine learning\n",
        "*   Test the model on the testing set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brVynfL2E9Kq",
        "colab_type": "code",
        "outputId": "8e0203bc-802f-4e08-98fe-4bda440a14be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df = pd.read_csv('creditcard.csv',low_memory=False)\n",
        "df = df.sample(frac=1).reset_index(drop=False)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>235030</td>\n",
              "      <td>148212.0</td>\n",
              "      <td>1.893193</td>\n",
              "      <td>-0.356911</td>\n",
              "      <td>-0.284687</td>\n",
              "      <td>0.482749</td>\n",
              "      <td>-0.640921</td>\n",
              "      <td>-0.631686</td>\n",
              "      <td>-0.335358</td>\n",
              "      <td>-0.136799</td>\n",
              "      <td>1.111946</td>\n",
              "      <td>-0.222126</td>\n",
              "      <td>-0.577131</td>\n",
              "      <td>1.117826</td>\n",
              "      <td>1.162727</td>\n",
              "      <td>-0.232646</td>\n",
              "      <td>0.586840</td>\n",
              "      <td>0.088636</td>\n",
              "      <td>-0.545089</td>\n",
              "      <td>-0.406828</td>\n",
              "      <td>-0.168637</td>\n",
              "      <td>-0.078774</td>\n",
              "      <td>-0.136022</td>\n",
              "      <td>-0.275964</td>\n",
              "      <td>0.328971</td>\n",
              "      <td>-0.026586</td>\n",
              "      <td>-0.423279</td>\n",
              "      <td>-0.627261</td>\n",
              "      <td>0.037245</td>\n",
              "      <td>-0.021455</td>\n",
              "      <td>49.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>239303</td>\n",
              "      <td>150041.0</td>\n",
              "      <td>2.112900</td>\n",
              "      <td>0.708255</td>\n",
              "      <td>-3.316643</td>\n",
              "      <td>0.659300</td>\n",
              "      <td>1.190690</td>\n",
              "      <td>-2.214111</td>\n",
              "      <td>1.184025</td>\n",
              "      <td>-0.671180</td>\n",
              "      <td>0.152928</td>\n",
              "      <td>-0.989553</td>\n",
              "      <td>-0.080917</td>\n",
              "      <td>-0.310265</td>\n",
              "      <td>-0.749395</td>\n",
              "      <td>-1.372961</td>\n",
              "      <td>0.275524</td>\n",
              "      <td>-0.446114</td>\n",
              "      <td>1.813585</td>\n",
              "      <td>0.548995</td>\n",
              "      <td>-0.280904</td>\n",
              "      <td>-0.254579</td>\n",
              "      <td>0.176506</td>\n",
              "      <td>0.773730</td>\n",
              "      <td>-0.263414</td>\n",
              "      <td>-0.158425</td>\n",
              "      <td>0.879745</td>\n",
              "      <td>-0.189920</td>\n",
              "      <td>-0.021938</td>\n",
              "      <td>-0.031027</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>120468</td>\n",
              "      <td>75839.0</td>\n",
              "      <td>-0.635995</td>\n",
              "      <td>0.937224</td>\n",
              "      <td>0.894019</td>\n",
              "      <td>-0.282678</td>\n",
              "      <td>0.870458</td>\n",
              "      <td>0.258952</td>\n",
              "      <td>0.523983</td>\n",
              "      <td>0.183416</td>\n",
              "      <td>-0.652047</td>\n",
              "      <td>-0.292577</td>\n",
              "      <td>1.094039</td>\n",
              "      <td>0.878936</td>\n",
              "      <td>1.058824</td>\n",
              "      <td>-0.472220</td>\n",
              "      <td>0.431214</td>\n",
              "      <td>0.492852</td>\n",
              "      <td>-0.341012</td>\n",
              "      <td>0.080424</td>\n",
              "      <td>0.179970</td>\n",
              "      <td>0.133684</td>\n",
              "      <td>-0.227255</td>\n",
              "      <td>-0.533720</td>\n",
              "      <td>0.054649</td>\n",
              "      <td>-0.877740</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>0.129143</td>\n",
              "      <td>0.151645</td>\n",
              "      <td>0.056006</td>\n",
              "      <td>2.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>239372</td>\n",
              "      <td>150075.0</td>\n",
              "      <td>0.067551</td>\n",
              "      <td>0.601325</td>\n",
              "      <td>1.317464</td>\n",
              "      <td>4.529598</td>\n",
              "      <td>0.301858</td>\n",
              "      <td>1.041750</td>\n",
              "      <td>-0.277797</td>\n",
              "      <td>0.373576</td>\n",
              "      <td>-1.386291</td>\n",
              "      <td>1.750149</td>\n",
              "      <td>-0.141688</td>\n",
              "      <td>-0.483083</td>\n",
              "      <td>-0.583645</td>\n",
              "      <td>0.234295</td>\n",
              "      <td>-0.124890</td>\n",
              "      <td>0.357175</td>\n",
              "      <td>-0.307992</td>\n",
              "      <td>1.277513</td>\n",
              "      <td>1.301217</td>\n",
              "      <td>0.241543</td>\n",
              "      <td>0.465484</td>\n",
              "      <td>1.338635</td>\n",
              "      <td>0.196847</td>\n",
              "      <td>-0.344737</td>\n",
              "      <td>-1.589232</td>\n",
              "      <td>0.312962</td>\n",
              "      <td>0.334584</td>\n",
              "      <td>0.302698</td>\n",
              "      <td>45.06</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>239333</td>\n",
              "      <td>150051.0</td>\n",
              "      <td>1.898510</td>\n",
              "      <td>-0.922520</td>\n",
              "      <td>-0.649560</td>\n",
              "      <td>-0.798661</td>\n",
              "      <td>-0.721101</td>\n",
              "      <td>-0.064760</td>\n",
              "      <td>-0.787303</td>\n",
              "      <td>0.146956</td>\n",
              "      <td>1.522735</td>\n",
              "      <td>-0.289679</td>\n",
              "      <td>0.783782</td>\n",
              "      <td>0.959655</td>\n",
              "      <td>-0.339569</td>\n",
              "      <td>0.051669</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>0.174936</td>\n",
              "      <td>-0.521479</td>\n",
              "      <td>0.500715</td>\n",
              "      <td>0.439921</td>\n",
              "      <td>-0.058134</td>\n",
              "      <td>0.266459</td>\n",
              "      <td>0.836335</td>\n",
              "      <td>0.114063</td>\n",
              "      <td>0.736882</td>\n",
              "      <td>-0.290676</td>\n",
              "      <td>0.729831</td>\n",
              "      <td>-0.051259</td>\n",
              "      <td>-0.050294</td>\n",
              "      <td>59.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index      Time        V1        V2  ...       V27       V28  Amount  Class\n",
              "0  235030  148212.0  1.893193 -0.356911  ...  0.037245 -0.021455   49.99      0\n",
              "1  239303  150041.0  2.112900  0.708255  ... -0.021938 -0.031027    1.00      0\n",
              "2  120468   75839.0 -0.635995  0.937224  ...  0.151645  0.056006    2.58      0\n",
              "3  239372  150075.0  0.067551  0.601325  ...  0.334584  0.302698   45.06      0\n",
              "4  239333  150051.0  1.898510 -0.922520  ... -0.051259 -0.050294   59.50      0\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__d36e8UvK-p",
        "colab_type": "code",
        "outputId": "b9809ee8-2a3e-4e11-9f64-aa66d818d77e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "frauds = df.loc[df['Class']==1]\n",
        "non_frauds=df.loc[df['Class']==0]\n",
        "print(len(frauds))\n",
        "print(len(non_frauds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "492\n",
            "284315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3wUe6TnOIA_",
        "colab_type": "code",
        "outputId": "e39bcda3-45d1-4328-b9bf-27056aff3bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "ax=frauds.plot.scatter(x='Amount', y='Class',color ='red',label='Fraud')\n",
        "non_frauds.plot.scatter(x=\"Amount\", y=\"Class\",color=\"green\",label =\"Non-Fraud\",ax=ax)\n",
        "plt.show()\n",
        "\n",
        "#note that this plot gives us the intution of the amount vs fraud relation. \n",
        "#Inference:\n",
        "#\n",
        "# Most of the cases are classified fraudulent when the amount is small\n",
        "#this can be visualizedin the scatter plot\n",
        "# \n",
        "\n",
        "#for the assignment you should make different graphs using barplot, saeaborn or pyplot or etc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/0i5mN5DVtMsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSiGtpfJuWWb",
        "colab_type": "text"
      },
      "source": [
        "- how to handle clear case of class imbalance?\n",
        "\n",
        "Sirajs go to methodology is adopting \"Clustering\". but surely there are other methods that one can adopt\n",
        "\n",
        "- How to decide whether to perform logistic regression next?\n",
        "\n",
        "Go to google and search for  images find the one that differentiates linear and logistic regression.\n",
        "\n",
        "With scikit learn we could bild 7 models in  7 lines of code and have them all trained in parallel.And look at all different accuracy scores and see which one is better at predictions. Unfortunately we dont have any sure shot way as to why some models work better than the others. That is where the math knowledge comes in to play.\n",
        "\n",
        "Since the data is imbalances , we believe that logistic regression is better than linear regression for the fraud detection problem.if the classes were pretty evenly split, liner would be better.\n",
        "\n",
        " - Get used to AutoML , later in the course\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP29b6ZUxNSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5WEMqEYP_kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=df.iloc[:, :-1]\n",
        "y= df[\"Class\"]\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.35)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cadFWHOxxwqE",
        "colab_type": "code",
        "outputId": "881e2e11-53ff-4bcd-cd46-059dc9542d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "logistic= linear_model.LogisticRegression(C=1e5)  #100,000\n",
        "#c is the penalty statistical term ofr over fitting\n",
        "# read more about it \n",
        "\n",
        "logistic.fit(X_train,y_train)\n",
        "print(\"score:\", logistic.score(X_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "score: 0.9987861521021638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZfMsWbwzy2s",
        "colab_type": "code",
        "outputId": "9843d09f-4cd6-4c08-dc05-e7601ff4d151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_predicted=np.array(logistic.predict(X_test))\n",
        "print(y_predicted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3IyXH1m1fTx",
        "colab_type": "text"
      },
      "source": [
        " * to check if its overitting , plot the graph and just check if the line is ltrying to fit across all or most of sample points, if not then it's probably not overfitting. \n",
        "\n",
        "        You could check different models and check which one gives you the most optimal results. Try it on the real world data.\n",
        "\n",
        "\n",
        "* How to get the correct value for hyperparameter \"c\" in logistic regression? Ths can be done throughr regularization- it attempts to engineer the model paramters such that it prevents overfitting for all the ML problems.\n",
        "\n",
        "\n",
        "      There is actually a lot of theory on  it. (quora)\n",
        "\n",
        "Q. What is the stripe alternative for countries that don't have stripe. ?\n",
        "- rasorpay - India\n",
        "- braintree\n",
        "- paypay\n",
        "\n",
        "\n",
        "Q. How to learn things?\n",
        "  * 4-5 hours a day. Get to the point that you could probably learn anything in just 2 days or a day.\n",
        "\n",
        "  * Turn off social media, just be alone. Immerse yourself in the world of topic that you are trying to laern- Focus- you will larn that fast\n",
        "\n",
        "\n",
        "* For those who graduate the course thay will be referred.\n",
        "\n",
        "How to you beat create datasets/\n",
        "Pandas- yiu will want it to clearly label all the columns,remove all the empty values from the dataset.\n",
        "\n",
        "Think about about the mid-term project. You will be paired to create a startup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEMq6piY0e0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}